# MADI Project
### *Local AI. Emotional Education. Human-Centered Technology.*
<img width="5118" height="2878" alt="7" src="https://github.com/user-attachments/assets/243e2be6-68c6-4096-a4af-362cf7cec3a6" />


---

## What is MADI?
**MADI** is an educational assistant built with **local LLM inference** and **TinyML emotional detection**, designed to bring high-quality learning support to communities without reliable internet access.

MADI listens, understands emotions through voice (neutral vs. frustrated), and responds with empathy â€” all running **offline**, on a  **Arduino Nano 33 BLE Sense** + **Jetson Orin Nano Super**.

No cloud. No data leaks.  
Just human-centered AI.

This project showcases how **Edge Impulse ML models** can power real-time **emotion recognition** on microcontrollers and how this emotional signal can be used to adapt the behavior of a **local LLM** running on embedded hardware.  
Emotion detection is the core innovation of this project and the main driver of the assistantâ€™s teaching style.


---

## ğŸ§  Core Features
- **TinyML-based emotion detection** on-device (audio / Edge Impulse)
- **Local Large Language Model** (Gemma 2B / Llama.cpp)  
- **Adaptive responses** depending on emotional state  
- **Push-to-talk hardware interaction**  
- **Offline, privacy-preserving operation**  
- **Designed for low-resource schools and communities**

---

## ğŸ”§ Hardware Used
- Arduino Nano 33 BLE Sense 
- NVIDIA Jetson Orin Nano Super  
- Speaker 4-Mic Array or USB mic  
- Bluetooth speaker (optional)  
- Physical button + RGB LED  

---

## ğŸ—ï¸ Architecture Overview

1. **Arduino Nano 33 BLE Sense** captures audio and runs a TinyML classifier that outputs two states:  
   - Neutral  
   - Frustrated  

2. It sends JSON via serial:
   ```json
   {"id":"audio","negative":0.23,"neutral":0.77}


3. **The Jetson Orin Nano Super** receives the emotional probabilities and integrates them into the assistantâ€™s workflow:  

   - If the learner is **Neutral**:
      - Clear and direct explanations  
      - Normal pacing  
      - Concise responses  

   - If the learner is **Frustrated**:
      - Slower, calmer responses  
      - Step-by-step reasoning  
      - More examples, analogies, and micro-activities  
      - Supportive and empathetic tone  

    Emotion directly influences how the assistant teaches.

---

## **Local Whisper Transcription â€” Voice to Text Offline**

When the user presses the button and speaks, Whisper (running fully offline) transcribes the audio and sends clean text to the LLM.

---

## **Local LLM (Gemma 2B / Llama.cpp) â€” Context-Aware Response Generation**

The LLM uses both:  
- The **transcribed text**, and  
- The **emotional state**  

â€¦to generate a fully local, privacy-preserving, and emotionally aligned answer.

---

## **Audio Output â€” The Assistant Speaks Back**

The final response is spoken using a local TTS system through a USB or Bluetooth speaker.


## Installation

Refer to the documentation:

- [Arduino/Edge Impulse setup](docs/installation_guide_arduino.md)
- [Jetson setup](docs/installation_guide_jetson.md)
- [HW conection setup](docs/conection_guide.md)
  

## Repository Structure

```
/
â”œâ”€â”€ assistant.py               Main assistant script
â”œâ”€â”€ README.md                  Project documentation
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ installation_guide_jetson.md
â”‚   â”œâ”€â”€ installation_guide_arduino.md
â”‚   â””â”€â”€ conection_guide.md
â””â”€â”€ SpeechEmotionRecognition/
    â””â”€â”€ SpeechEmotionRecognition.ino

```




